{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Terraform Notes\n",
    "### This notebook contains helpful cmds and snippets for:\n",
    "* Initializing a terraform project and scripts necessary for provisioning GCP resources like GCS Buckets and BigQuery Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example terraform main.tf file (Responsible for provisioning various microservcies through a provider):\n",
    "\n",
    "```\n",
    "terraform {\n",
    "  required_version = \">= 1.0\"\n",
    "  backend \"local\" {}  # Can change from \"local\" to \"gcs\" (for google) or \"s3\" (for aws), if you would like to preserve your tf-state online\n",
    "  required_providers {\n",
    "    google = {\n",
    "      source  = \"hashicorp/google\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"google\" {\n",
    "  project = var.project\n",
    "  region = var.region\n",
    "  // credentials = file(var.credentials)  # Use this if you do not want to set env-var GOOGLE_APPLICATION_CREDENTIALS\n",
    "}\n",
    "\n",
    "# Data Lake Bucket\n",
    "# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_bucket\n",
    "resource \"google_storage_bucket\" \"data-lake-bucket\" {\n",
    "  name          = \"${local.data_lake_bucket}_${var.project}\" # Concatenating DL bucket & Project name for unique naming\n",
    "  location      = var.region\n",
    "\n",
    "  # Optional, but recommended settings:\n",
    "  storage_class = var.storage_class\n",
    "  uniform_bucket_level_access = true\n",
    "\n",
    "  versioning {\n",
    "    enabled     = true\n",
    "  }\n",
    "\n",
    "  lifecycle_rule {\n",
    "    action {\n",
    "      type = \"Delete\"\n",
    "    }\n",
    "    condition {\n",
    "      age = 30  // days\n",
    "    }\n",
    "  }\n",
    "\n",
    "  force_destroy = true\n",
    "}\n",
    "\n",
    "# DWH\n",
    "# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset\n",
    "resource \"google_bigquery_dataset\" \"dataset\" {\n",
    "  dataset_id = var.BQ_DATASET\n",
    "  project    = var.project\n",
    "  location   = var.region\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of variables.tf (specifies env variables, projectIDs, and other user specific information)\n",
    "\n",
    "```\n",
    "locals {\n",
    "  data_lake_bucket = \"dtc_data_lake\"\n",
    "}\n",
    "\n",
    "variable \"project\" {\n",
    "  description = \"Your GCP Project ID\"\n",
    "}\n",
    "\n",
    "variable \"region\" {\n",
    "  description = \"Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations\"\n",
    "  default = \"us-west1\"\n",
    "  type = string\n",
    "}\n",
    "\n",
    "variable \"storage_class\" {\n",
    "  description = \"Storage class type for your bucket. Check official docs for more info.\"\n",
    "  default = \"STANDARD\"\n",
    "}\n",
    "\n",
    "variable \"BQ_DATASET\" {\n",
    "  description = \"BigQuery Dataset that raw data (from GCS) will be written to\"\n",
    "  type = string\n",
    "  default = \"trips_data_all\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
